# -*- coding: utf-8 -*-
"""
Created on Mon Jan 20 14:34:56 2020

@author: 328576
"""

import re
import unicodedata
import nltk
import inflect
import en_core_web_sm

nlp = en_core_web_sm.load()


def concatenate_list_data(list):

    result = ''

    for element in list:

        text = str(element)
        result += text

    return result


def remove_non_ascii(words):
    """Remove non-ASCII characters from list of token words"""
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words


def to_lowercase(words):
    """Convert all characters to lowercase from list of token words"""
    new_words = []
    for word in words:
        word = str(word)
        new_word = word.lower()
        new_words.append(new_word)
    return new_words


def remove_n_character(words):
    """Remove tokens with 3 or less characters from list"""
    new_words = []
    for word in words:
        short_word = re.compile(r'\W*\b\w{1,3}\b')
        new_word = short_word.sub('', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words


def remove_punctuation(words):

    """Remove punctuation from list of token words"""

    new_words = []
    for word in words:
        new_word = re.sub(r'[^a-zA-Z0-9\s]', '', word)
        new_word = re.sub(r'[^\w\d\s]', '', new_word)
        new_word = re.sub('Â£', '', new_word)
        new_word = re.sub('%', '', new_word)

        if new_word != '':
            new_words.append(new_word)
    return new_words


def remove_spaces(words):

    """Remove spaces and n from list of token words"""

    new_words = []
    for word in words:
        new_word = re.sub('\n', '', word)
        new_word = new_word.rstrip()
        if new_word != '':
            new_words.append(new_word)
    return new_words


def replace_numbers_as_text(words):

    """Replace all integer occurrences in list of token words with textual representation"""

    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words


def replace_numbers(words):

    """Replace all integer occurrences in list of token words with textual representation"""

    new_words = []
    for word in words:
        new_word = re.sub(r'\d+', '', word)
        if new_word != '':
            new_words.append(new_word)

    return new_words



def remove_stopwords(words):

    """Remove stop words from list of token words"""

    new_words = []
    for word in words:
        if word not in nlp.Defaults.stop_words:
            new_words.append(word)
    return new_words

def stem_words(words):

    """Stem words in list of token words"""

    stemmer = nltk.LancasterStemmer()
    stems = []
    for word in words:
        stem = stemmer.stem(word)
        stems.append(stem)
    return stems


def lemmatize_verbs(words):

    """Lemmatize verbs in list of token words"""

    lemmatizer = nltk.WordNetLemmatizer()
    lemmas = []
    for word in words:
        lemma = lemmatizer.lemmatize(word, pos='v')
        lemmas.append(lemma)
    return lemmas


def normalize_words(words):
    """ function to normalise list of words generated by nlp"""
    document_clean = to_lowercase(words)
    document_clean = replace_numbers(document_clean)
    document_clean = remove_punctuation(document_clean)
    document_clean = remove_n_character(document_clean)
    document_clean = remove_spaces(document_clean)
    document_clean = remove_stopwords(document_clean)
    # document_clean = stem_words(document_clean)
    #  document_clean = lemmatize_verbs(document_clean) #  don't have access to this data

    if document_clean != '':
        print('There are ' + str(len(document_clean)) + " 'clean' words in the selected corpus of documents")

    return document_clean


def normalize_sentences(sentences):

    """ function to normalise list of sentences generated by nlp"""

    sentence_clean = to_lowercase(sentences)
    sentence_clean = remove_n_character(sentence_clean)
    sentence_clean = remove_punctuation(sentence_clean)

    if sentence_clean != '':
        print('There are ' + str(len(sentence_clean)) + " 'clean' sentences in the selected corpus of documents")

    return sentence_clean


def clean_corpus_words(text):
    """ function to normalise list of sentences generated by nlp"""

    corpus_clean = to_lowercase(text)
    corpus_clean = remove_n_character(corpus_clean)
    corpus_clean = replace_numbers(corpus_clean)


    if corpus_clean != '':
        print('There are ' + str(len(corpus_clean)) + " 'clean' documents in the selected corpus")

    return corpus_clean


def clean_corpus_sentences(text):
    """ function to normalise list of sentences generated by nlp"""

    corpus_clean = to_lowercase(text)
    corpus_clean = remove_n_character(corpus_clean)

    if corpus_clean != '':
        print('There are ' + str(len(corpus_clean)) + " 'clean' documents in the selected corpus")

    return corpus_clean

